{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43ea3c26",
   "metadata": {},
   "source": [
    "# Assignment 2: Training the Fully Recurrent Network\n",
    "\n",
    "*Author:* Thomas Adler\n",
    "\n",
    "*Copyright statement:* This  material,  no  matter  whether  in  printed  or  electronic  form,  may  be  used  for  personal  and non-commercial educational use only.  Any reproduction of this manuscript, no matter whether as a whole or in parts, no matter whether in printed or in electronic form, requires explicit prior acceptance of the authors.\n",
    "\n",
    "\n",
    "## Exercise 1: Data generation\n",
    "\n",
    "There are two classes, both occurring with probability 0.5. There is one input unit. Only the first sequence element conveys relevant information about the class. Sequence elements at positions $t > 1$ stem from a Gaussian with mean zero and variance 0.2. The first sequence element is 1.0 (-1.0) for class 1 (2). Target at sequence end is 1.0 (0.0) for class 1 (2)\n",
    "\n",
    "Write a function `generate_data` that takes an integer `T` as argument which represents the sequence length. Seed the `numpy` random generator with the number `0xDEADBEEF`. Implement the [Python3 generator](https://docs.python.org/3/glossary.html#term-generator) pattern and produce data in the way described above. The input sequences should have the shape `(T, 1)` and the target values should have the shape `(1,)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04244bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from scipy.special import expit as sigmoid\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class FullyRecurrentNetwork(object):\n",
    "    def __init__(self, D, I, K):\n",
    "        self.W = np.random.uniform(-0.01, 0.01, (I, D))\n",
    "        self.R = np.random.uniform(-0.01, 0.01, (I, I))\n",
    "        self.V = np.random.uniform(-0.01, 0.01, (K, I))\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        # helper function for numerically stable loss\n",
    "        def f(z):\n",
    "            return np.log1p(np.exp(-np.absolute(z))) + np.maximum(0, z)\n",
    "        \n",
    "        # infer dims\n",
    "        T, D = x.shape\n",
    "        K, I = self.V.shape\n",
    "\n",
    "        # init result arrays\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.a = np.zeros((T, I))\n",
    "        \n",
    "        # iterate forward in time \n",
    "        # trick: access model.a[-1] in first iteration\n",
    "        for t in range(T):\n",
    "            self.a[t] = np.tanh(self.W @ x[t] + self.R @ self.a[t-1])\n",
    "            \n",
    "        self.z = model.V @ self.a[t]\n",
    "        return y * f(-self.z) + (1-y) * f(self.z)\n",
    "\n",
    "T, D, I, K = 10, 3, 5, 1\n",
    "model = FullyRecurrentNetwork(D, I, K)\n",
    "model.forward(np.random.uniform(-1, 1, (T, D)), 1)\n",
    "\n",
    "import random \n",
    "\n",
    "def gen(T):\n",
    "    mu, sigma = 0, 0.2\n",
    "    for i in range(0,T):\n",
    "        yield np.random.normal(mu, sigma)\n",
    "\n",
    "\n",
    "def generate_data(T):\n",
    "    ########## YOUR SOLUTION HERE ##########\n",
    "    \n",
    "    x1 = np.fromiter(gen(T),dtype=float)\n",
    "    x2 = np.fromiter(gen(T),dtype=float)\n",
    "    \n",
    "    x1 = np.reshape(x1,newshape=(T,1))\n",
    "    x2 = np.reshape(x2,newshape=(T,1))\n",
    "\n",
    "    x1[0][0] = 1\n",
    "    y1 = np.array([1.0],dtype=float)\n",
    "\n",
    "    x2[0][0] = -1.0\n",
    "    y2 = np.array([0.0],dtype=float)\n",
    "\n",
    "    choice = np.random.choice([1,-1],1,p=[0.5,0.5])\n",
    "    if choice[0] == 1:\n",
    "        return x1,y1 #\n",
    "         \n",
    "    else:\n",
    "        return x2,y2 #\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abd7c2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0xDEADBEEF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "554c71f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape:  (5, 1)\n",
      "x[0]: [1.]  y[0]: 1.0\n",
      "y shape:  (1,)\n"
     ]
    }
   ],
   "source": [
    "x,y = generate_data(5)\n",
    "print(\"x shape: \",x.shape)\n",
    "print(\"x[0]:\",x[0],\" y[0]:\",y[0])\n",
    "print(\"y shape: \",y.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a9826f26",
   "metadata": {},
   "source": [
    "## Exercise 2: Gradients for the network parameters\n",
    "Compute gradients of the total loss \n",
    "$$\n",
    "L = \\sum_{t=1}^T L(t), \\quad \\text{where} \\quad L(t) = L(z(t), y(t))\n",
    "$$\n",
    "w.r.t. the weights of the fully recurrent network. To this end, find the derivative of the loss w.r.t. the logits and hidden pre-activations first, i.e., \n",
    "$$\n",
    "\\psi^\\top(t) = \\frac{\\partial L}{\\partial z(t)} \\quad \\text{and} \\quad \\delta^\\top(t) = \\frac{\\partial L}{\\partial s(t)}.\n",
    "$$\n",
    "With the help of these intermediate results you should be able to compute the gradients w.r.t. the weights, i.e., $\\nabla_W L, \\nabla_R L, \\nabla_V L$. \n",
    "\n",
    "*Hint: Take a look at the computational graph from the previous assignment to see the functional dependencies.*\n",
    "\n",
    "*Remark: Although we only have one label at the end of the sequence, we consider the more general case of evaluating a loss at every time step in this exercise (many-to-many mapping).*\n",
    "\n",
    "########## YOUR SOLUTION HERE #########\n",
    "\n",
    "Let's remember how forward pass looks like:\n",
    "$$\n",
    "\\vec{s}(t) = \\vec{W}^\\top \\vec{x}(t) + \\vec{R}^\\top \\vec{a}(t-1) \\\\\n",
    "\\vec{a}(t) = f(\\vec{s}(t)) \\\\\n",
    "\\vec{z}(t) = \\vec{V}^\\top \\vec{a}(t) \\\\\n",
    "\\hat{\\vec{y}}(t) = \\phi(\\vec{z})\n",
    "$$\n",
    ", where $f$ is an $tanh$ - activation function and $\\phi$ is $sigmoid$ - output activation function, which depends on the task (not on the architecture)\n",
    "\n",
    "For the convinience of the future calculations we have to define derivative of the loss w.r.t. pre-activation function $\\vec{s}(t)$, which is called delta-error $\\vec{\\delta}$.\n",
    " \n",
    "Delta can be defined as following:\n",
    "$$\n",
    "\\vec{\\delta}(t)^\\top = \\frac{\\partial \\vec{L}}{\\partial \\vec{s}(t)} = \n",
    "\\frac{\\partial\\vec{L}}{\\partial\\vec{a}(t)}\\frac{\\partial\\vec{a}(t)}{\\partial\\vec{s}(t)} = \\\\ [20pt]\n",
    "\n",
    "\\left(\\frac{\\partial\\vec{L}\\left(\\vec{y}(t),\\hat{\\vec{y}}(t)\\right)}{\\partial\\vec{a}(t)} + \\frac{\\partial\\vec{L}}{\\partial\\vec{s}(t+1)} \\frac{\\partial\\vec{s}(t+1)}{\\partial\\vec{a}(t)}\\right)\\frac{\\partial{\\vec{a}(t)}}{\\partial{\\vec{s}(t)}} = \\\\[20pt]\n",
    "\n",
    "\\left(\\frac{\\partial\\vec{L}}{\\partial\\hat{\\vec{y}}(t)}\\frac{\\partial\\hat{\\vec{y}}(t)}{\\partial\\vec{z}(t)}\\frac{\\partial\\vec{z}(t)}{\\partial\\vec{a}(t)} + \\frac{\\partial\\vec{L}}{\\partial\\vec{s}(t+1)} \\frac{\\partial\\vec{s}(t+1)}{\\partial\\vec{a}(t)}\\right)\\frac{\\partial{\\vec{a}(t)}}{\\partial{\\vec{s}(t)}}\\\\ [20pt]\n",
    "$$\n",
    "$\\\\$ Here where it becomes tricky. From the last assignment, we remember that in practice, instead of separate prediction value $\\hat{\\vec{y}}$,  *logit*  $\\vec{z}(t)$ is directly inserted into loss function: $\\\\ \\vec{L}_{\\text{BCE stable}}(\\vec{z}(t), \\vec{y}(t)) =  \\log(1+e^{-|z|})+\\max(0, z)(1-y)$ . \n",
    "\n",
    "So, $\\frac{\\partial\\vec{L}}{\\partial\\hat{\\vec{y}}(t)}\\frac{\\partial\\hat{\\vec{y}}(t)}{\\partial\\vec{z}(t)}$ can be simplified to: $\\frac{\\partial L_{\\text{BCE stable}}(\\vec{z}(t), \\vec{y}(t))}{\\partial \\vec{z}(t)}$, which equals to:\n",
    "\n",
    "$$ \n",
    "\\frac{\\partial L_{\\text{BCE stable}}(\\vec{z}(t), \\vec{y}(t))}{\\partial \\vec{z}(t)} = \\frac{\\partial \\log(1+e^{-|\\vec{z}(t)|})+\\max(0, \\vec{z}(t))(1-y)}{\\partial \\vec{\\vec{z}(t)}} = - \\frac{sgn(\\vec{z}(t))}{1+e^{\\vec{z}(t)}} + \\frac{\\max(0, \\vec{z}(t))}{\\vec{z}(t)}(1-y)\n",
    "$$\n",
    "We will not, however, put that whole expression into our calculation of delta, but for the sake of simplicity let it be written  as $\\psi(t)^\\top$ . Therefore:\n",
    "\n",
    "$$\n",
    "... \n",
    "\\left(\\frac{\\partial L_{\\text{BCE stable}}(\\vec{z}(t), \\vec{y}(t))}{\\partial \\vec{z}(t)}\\frac{\\partial\\vec{z}(t)}{\\partial\\vec{a}(t)} + \\frac{\\partial\\vec{L}}{\\partial\\vec{s}(t+1)} \\frac{\\partial\\vec{s}(t+1)}{\\partial\\vec{a}(t)}\\right)\\frac{\\partial{\\vec{a}(t)}}{\\partial{\\vec{s}(t)}} = \\\\ [20pt]\n",
    "\n",
    "\\left( \\psi(t)^\\top \\vec{V}^\\top + \\vec{\\delta}(t+1)^\\top \\vec{R}^\\top \\right)diag\\left(f^\\prime(\\vec{s}(t)) \\right) \\\\[20pt]\n",
    "$$\n",
    "Whereas $f^\\prime(\\vec{s}(t)) = \\frac{\\partial\\, tanh(\\vec{s}(t)) }{\\partial\\, \\vec{s}(t)} = \\frac{1}{cosh^2(\\vec{s}(t))}$\n",
    "\n",
    "Using delta we can easily calculate gradients w.r.t. weights:\n",
    "$$\n",
    "\\nabla_{\\vec{R}} \\vec{L} = \\sum_{t=1}^T \\frac{\\partial \\vec{L}}{\\partial\\vec{s}(t)}\\frac{\\partial\\vec{s}(t)}{\\partial\\vec{R}} = \\sum_{t=1}^T \\vec{\\delta}(t)^\\top \\frac{\\partial \\left(\\vec{W}^\\top \\vec{x}(t) + \\vec{R}^\\top \\vec{a}(t-1) \\right)}{\\partial \\vec{R}} =\n",
    "\\sum_{t=1}^T \\vec{\\delta}(t)^\\top \\vec{a}(t-1) \\\\ [20pt]\n",
    "\n",
    "\\nabla_{\\vec{W}} \\vec{L} =  \\sum_{t=1}^T \\frac{\\partial \\vec{L}}{\\partial\\vec{s}(t)}\\frac{\\partial\\vec{s}(t)}{\\partial\\vec{W}} = \\sum_{t=1}^T \\vec{\\delta}(t)^\\top \\frac{\\partial \\left(\\vec{W}^\\top \\vec{x}(t) + \\vec{R}^\\top \\vec{a}(t-1) \\right)}{\\partial \\vec{W}} =\n",
    "\\sum_{t=1}^T \\vec{\\delta}(t)^\\top \\vec{x}(t) \\\\ [20pt]\n",
    "$$\n",
    "\n",
    "To find gradient for $\\vec{V}$ we shouldn't fo deep down untill $\\vec{s}(t)$, because $\\vec{V}$ can calculated using $\\psi(t)^\\top$ .  \n",
    "$$\n",
    "\\vec{\\psi}^\\top(t) = \\frac{\\partial L_{\\text{BCE stable}}(\\vec{z}(t), \\vec{y}(t))}{\\partial \\vec{z}(t)} = \\frac{\\partial L_{\\text{BCE stable}}(\\vec{z}(t), \\vec{y}(t))}{\\partial\\vec{V}^\\top\\; \\vec{a}(t)}\n",
    "$$\n",
    "\n",
    "\n",
    "Multiplying this definition with $\\vec{a}(t)$ will cancel out $\\vec{a}(t)$ in denominator:\n",
    "$$\n",
    "\\nabla_{\\vec{V}} \\vec{L} = \\sum_{t=1}^T \\frac{\\partial L_{\\text{BCE stable}}(\\vec{z}(t), \\vec{y}(t))}{\\partial\\vec{V}^\\top\\; \\cancel{\\vec{a}(t)}} \\cancel{\\;\\vec{a}(t)} = \\sum_{t=1}^T \\vec{\\psi}(t)\\;\\vec{a}(t)^\\top\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21968f2c",
   "metadata": {},
   "source": [
    "## Exercise 3: The backward pass\n",
    "Write a function `backward` that takes a model `self` as argument. The function should compute the gradients of the loss with respect to all model parameters and store them to `self.dW`, `self.dR`, `self.dV`, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42e3d13c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/numpy/core/fromnumeric.py:87: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
     ]
    }
   ],
   "source": [
    "def backward(self): \n",
    "    delta = np.zeros((T,I))\n",
    "    self.gradR = 0\n",
    "    self.gradW = 0\n",
    "    self.gradV = 0\n",
    "    dL_da = np.zeros((T,I))\n",
    "\n",
    "    \n",
    "    # calculate psi[t]\n",
    "    # Loss formula was directly taken from forward pass   #\n",
    "    psi = - (np.sign(self.z))/(np.exp(np.abs(self.z))+1) \\\n",
    "          + (np.max([0,self.z])/self.z)*(1-self.y) \n",
    "    # derivative of the unstable bce loss    \n",
    "    # psi = (np.exp(self.z)*(self.y-1)+self.y) / (np.exp(self.z)+1)              \n",
    "                                    \n",
    "\n",
    "    # just before start looping, calculate dL_da for the last position once using psi,\n",
    "    # since we won't have psi[t] anymore\n",
    "    dL_da[-1] = psi.reshape(1,1) @ self.V\n",
    "\n",
    "    # calculate delta[t]\n",
    "    # delta = dL/da * da/ds\n",
    "    for t in reversed(range(0, T)):\n",
    "\n",
    "        # complete dL_da in case we've got already delta[t+1], i.e. we aren't at the end     \n",
    "        if t < (T-1): \n",
    "            dL_da[t] += delta[t+1] @ self.R \n",
    "\n",
    "        # calculate s_t for diag(f's(t)), i.e. dor da/ds\n",
    "        s_t = self.W @ self.x[t].reshape(D,1)\n",
    "        # complete s_t for two cases: t>0 and t==0\n",
    "        if t > 0:\n",
    "            s_t += self.R @ self.a[t-1].reshape(I,1)\n",
    "        else:\n",
    "            s_t += self.R @ np.zeros_like(self.a[t]).reshape(I,1)\n",
    "        # insert s_t into formula, so we have da_ds\n",
    "        da_ds = np.diag(1/(np.cosh(s_t))**2)\n",
    "        \n",
    "        # finally, calculate delta\n",
    "        delta[t] = dL_da[t].reshape(I,1) @ da_ds\n",
    "    \n",
    "    # calculate gradients \n",
    "    for t in range(0, T):\n",
    "        #calculate self.gradR\n",
    "        if t!=0:       \n",
    "            self.gradR += delta[t].reshape(I,1) @ self.a[t-1].reshape(1,I)\n",
    "        # calculate gradW\n",
    "        self.gradW += delta[t].reshape(I,1) @ self.x[t].reshape(1,D)\n",
    "        # calculate self.gradV\n",
    "        if len(psi)!=1:\n",
    "            self.gradV += psi[t].reshape(K,K) @ self.a[t].reshape(K,I)\n",
    "    if len(psi) == 1:\n",
    "        self.gradV += psi[-1].reshape(K,K) @ self.a[-1].reshape(K,I)\n",
    "    \n",
    "FullyRecurrentNetwork.backward = backward\n",
    "model.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58181c6",
   "metadata": {},
   "source": [
    "## Exercise 4: Gradient checking\n",
    "Write a function `grad_check` that takes a model `self`, a float `eps` and another float `thresh` as arguments and computes the numerical gradients of the model parameters according to the approximation\n",
    "$$\n",
    "f'(x) \\approx \\frac{f(x + \\varepsilon) - f(x - \\varepsilon)}{2 \\varepsilon}.\n",
    "$$\n",
    "If any of the analytical gradients are farther than `thresh` away from the numerical gradients the function should throw an error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "227e8631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad approx-on:  0.008927042438600097 \n",
      "\n",
      "checking gradients for V\n",
      "Error! diff > theshold\n",
      "dV: 0.005153439937935269, diff: 0.2680023595607353\n",
      "Error! diff > theshold\n",
      "dV: -0.004136782055520702, diff: 1.0\n",
      "Error! diff > theshold\n",
      "dV: 0.00300664424621959, diff: 0.4961080635635912\n",
      "Error! diff > theshold\n",
      "dV: 0.0029608825404280237, diff: 0.5018672231442555\n",
      "Error! diff > theshold\n",
      "dV: 0.001441226211709417, diff: 0.7219928880476311\n",
      "checking gradients for W\n",
      "Error! diff > theshold\n",
      "dV: 0.001441226211709417, diff: 0.8292190161907116\n",
      "Error! diff > theshold\n",
      "dV: 0.001441226211709417, diff: 0.4648925337287585\n",
      "Error! diff > theshold\n",
      "dV: 0.001441226211709417, diff: 1.0\n",
      "checking gradients for W\n",
      "Error! diff > theshold\n",
      "dV: 0.001441226211709417, diff: 0.9137909575346039\n",
      "Error! diff > theshold\n",
      "dV: 0.001441226211709417, diff: 0.7025474727400177\n",
      "Error! diff > theshold\n",
      "dV: 0.001441226211709417, diff: 1.0\n",
      "checking gradients for W\n",
      "Error! diff > theshold\n",
      "dV: 0.001441226211709417, diff: 1.0\n",
      "Error! diff > theshold\n",
      "dV: 0.001441226211709417, diff: 1.0\n",
      "Error! diff > theshold\n",
      "dV: 0.001441226211709417, diff: 0.7578036898861196\n",
      "checking gradients for W\n",
      "Error! diff > theshold\n",
      "dV: 0.001441226211709417, diff: 1.0\n",
      "Error! diff > theshold\n",
      "dV: 0.001441226211709417, diff: 1.0\n",
      "Error! diff > theshold\n",
      "dV: 0.001441226211709417, diff: 0.9078854925156021\n",
      "checking gradients for W\n",
      "Error! diff > theshold\n",
      "dV: 0.001441226211709417, diff: 1.0\n",
      "Error! diff > theshold\n",
      "dV: 0.001441226211709417, diff: 1.0\n",
      "Error! diff > theshold\n",
      "dV: 0.001441226211709417, diff: 0.9855465989621568\n",
      "checking gradients for R\n",
      "Error! diff > theshold\n",
      "dR: -5.780720785772844e-06, diff: 1.0\n",
      "Error! diff > theshold\n",
      "dR: 1.4104026484177425e-05, diff: 0.9968451413833217\n",
      "Error! diff > theshold\n",
      "dR: -6.306368778474731e-06, diff: 1.0\n",
      "Error! diff > theshold\n",
      "dR: -7.556479180504472e-06, diff: 1.0\n",
      "Error! diff > theshold\n",
      "dR: -9.413213931601867e-06, diff: 1.0\n",
      "checking gradients for R\n",
      "Error! diff > theshold\n",
      "dR: -2.740816699454175e-06, diff: 1.0\n",
      "Error! diff > theshold\n",
      "dR: 6.8228784700601795e-06, diff: 0.9984725808532117\n",
      "Error! diff > theshold\n",
      "dR: -3.0944948793171435e-06, diff: 1.0\n",
      "Error! diff > theshold\n",
      "dR: -3.7141735779170293e-06, diff: 1.0\n",
      "Error! diff > theshold\n",
      "dR: -4.456386140376877e-06, diff: 1.0\n",
      "checking gradients for R\n",
      "Error! diff > theshold\n",
      "dR: 5.4559088066470134e-06, diff: 0.9987784136991793\n",
      "Error! diff > theshold\n",
      "dR: -1.2874921636925434e-05, diff: 1.0\n",
      "Error! diff > theshold\n",
      "dR: 5.61836176754263e-06, diff: 0.9987420631112937\n",
      "Error! diff > theshold\n",
      "dR: 6.7123439502935304e-06, diff: 0.9984973073217982\n",
      "Error! diff > theshold\n",
      "dR: 8.90269857020908e-06, diff: 0.9980074410857389\n",
      "checking gradients for R\n",
      "Error! diff > theshold\n",
      "dR: 1.9057637177813758e-06, diff: 0.9995731269406878\n",
      "Error! diff > theshold\n",
      "dR: -4.6114805721351315e-06, diff: 1.0\n",
      "Error! diff > theshold\n",
      "dR: 2.0511010184610983e-06, diff: 0.9995405802370958\n",
      "Error! diff > theshold\n",
      "dR: 2.456167394045936e-06, diff: 0.9994498756307779\n",
      "Error! diff > theshold\n",
      "dR: 3.1031628059034198e-06, diff: 0.9993050140626117\n",
      "checking gradients for R\n",
      "Error! diff > theshold\n",
      "dR: 2.8639388888041243e-07, diff: 0.9999358388395333\n",
      "Error! diff > theshold\n",
      "dR: -7.205721996641826e-07, diff: 1.0\n",
      "Error! diff > theshold\n",
      "dR: 3.293124107087454e-07, diff: 0.9999262241071856\n",
      "Error! diff > theshold\n",
      "dR: 3.9561014224058046e-07, diff: 0.999911372077839\n",
      "Error! diff > theshold\n",
      "dR: 4.651585770657476e-07, diff: 0.9998957920624535\n"
     ]
    }
   ],
   "source": [
    "def grad_check(self, eps, thresh):\n",
    "    ########## YOUR SOLUTION HERE ##########\n",
    "    self.eps = eps\n",
    "    self.thresh = thresh\n",
    "\n",
    "    # add eps to all weights\n",
    "    self.W = self.W + self.eps\n",
    "    self.V = self.V + self.eps\n",
    "    self.R = self.R + self.eps\n",
    "    #call forward() to make forward pass\n",
    "    lossPlusEps = self.forward(self.x,self.y)\n",
    "\n",
    "    # minus 2eps from to prev added weights, to make it x-e\n",
    "    self.W = self.W - 2*self.eps\n",
    "    self.V = self.V - 2*self.eps\n",
    "    self.R = self.R - 2*self.eps\n",
    "    # call forward() to make another forward pass\n",
    "    lossMinusEps = self.forward(self.x,self.y)\n",
    "    \n",
    "    # calculate numerical gradient\n",
    "    gradApprox = ((lossPlusEps - lossMinusEps)/(2*self.eps))[0]\n",
    "    print(\"grad approx-on: \", gradApprox,\"\\n\")\n",
    "    for row in self.gradV:\n",
    "        print(\"checking gradients for V\")\n",
    "        for dV in row:\n",
    "            diff = np.abs(gradApprox - dV) \n",
    "            diff = np.linalg.norm(gradApprox-dV)/(np.linalg.norm(dV)+np.linalg.norm(gradApprox))\n",
    "            if diff > self.thresh:\n",
    "                print(\"Error! diff > theshold\")\n",
    "                print(f\"dV: {dV}, diff: {diff}\")\n",
    "    for row in self.gradW:\n",
    "        print(\"checking gradients for W\")\n",
    "        for dW in row:\n",
    "            diff = np.linalg.norm(-dW+gradApprox)/(np.linalg.norm(dW)+np.linalg.norm(gradApprox))\n",
    "            if diff > self.thresh:\n",
    "                print(\"Error! diff > theshold\")\n",
    "                print(f\"dV: {dV}, diff: {diff}\")\n",
    "    for row in self.gradR:\n",
    "        print(\"checking gradients for R\")\n",
    "        for dR in row:\n",
    "            diff = np.linalg.norm(-dR+gradApprox)/(np.linalg.norm(dR)+np.linalg.norm(gradApprox))\n",
    "            if diff > self.thresh:\n",
    "                print(\"Error! diff > theshold\")\n",
    "                print(f\"dR: {dR}, diff: {diff}\")\n",
    "   \n",
    "    # restore model weights to the default ones\n",
    "    self.W = self.W + self.eps\n",
    "    self.V = self.V + self.eps\n",
    "    self.R = self.R + self.eps\n",
    "\n",
    "FullyRecurrentNetwork.grad_check = grad_check\n",
    "model.grad_check(1e-7, 1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5cdc05",
   "metadata": {},
   "source": [
    "## Exercise 5: Parameter update\n",
    "\n",
    "Write a function `update` that takes a model `self` and a float argument `eta`, which represents the learning rate. The method should implement the gradient descent update rule $\\theta \\gets \\theta - \\eta \\nabla_{\\theta}L$ for all model parameters $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e93c02c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(self, eta):\n",
    "    self.W -= eta * self.gradW\n",
    "    self.V -= eta * self.gradV\n",
    "    self.R -= eta * self.gradR   \n",
    "\n",
    "FullyRecurrentNetwork.update = update\n",
    "model.update(0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19801bb8",
   "metadata": {},
   "source": [
    "## Exercise 6: Network training\n",
    "\n",
    "Train the fully recurrent network with 32 hidden units. Start with input sequences of length one and tune the learning rate and the number of update steps. Then increase the sequence length by one and tune the hyperparameters again. What is the maximal sequence length for which the fully recurrent network can achieve a performance that is better than random? Visualize your results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9781ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## YOUR SOLUTION HERE ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b52057",
   "metadata": {},
   "source": [
    "## Exercise 7: The Vanishing Gradient Problem\n",
    "\n",
    "Analyze why the network is incapable of learning long-term dependencies. Show that $\\|\\frac{\\partial a(T)}{\\partial a(1)}\\|_2 \\leq \\|R\\|_2^{T-1}$ , where $\\|\\cdot\\|_2$ is the spectral norm, and discuss how that affects the propagation of error signals through the time dimension of the network. \n",
    "\n",
    "*Hint: Use the fact that the spectral norm is submultiplicative for square matrices, i.e. $\\|AB\\|_2 \\leq \\|A\\|_2\\|B\\|_2$ if $A$ and $B$ are both square.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158130a5",
   "metadata": {},
   "source": [
    "########## YOUR SOLUTION HERE ##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e50719",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
