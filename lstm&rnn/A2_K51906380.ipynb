{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43ea3c26",
   "metadata": {},
   "source": [
    "# Assignment 2: Training the Fully Recurrent Network\n",
    "\n",
    "*Author:* Thomas Adler\n",
    "\n",
    "*Copyright statement:* This  material,  no  matter  whether  in  printed  or  electronic  form,  may  be  used  for  personal  and non-commercial educational use only.  Any reproduction of this manuscript, no matter whether as a whole or in parts, no matter whether in printed or in electronic form, requires explicit prior acceptance of the authors.\n",
    "\n",
    "\n",
    "## Exercise 1: Data generation\n",
    "\n",
    "There are two classes, both occurring with probability 0.5. There is one input unit. Only the first sequence element conveys relevant information about the class. Sequence elements at positions $t > 1$ stem from a Gaussian with mean zero and variance 0.2. The first sequence element is 1.0 (-1.0) for class 1 (2). Target at sequence end is 1.0 (0.0) for class 1 (2)\n",
    "\n",
    "Write a function `generate_data` that takes an integer `T` as argument which represents the sequence length. Seed the `numpy` random generator with the number `0xDEADBEEF`. Implement the [Python3 generator](https://docs.python.org/3/glossary.html#term-generator) pattern and produce data in the way described above. The input sequences should have the shape `(T, 1)` and the target values should have the shape `(1,)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04244bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from scipy.special import expit as sigmoid\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class FullyRecurrentNetwork(object):\n",
    "    def __init__(self, D, I, K):\n",
    "        self.W = np.random.uniform(-0.01, 0.01, (I, D))\n",
    "        self.R = np.random.uniform(-0.01, 0.01, (I, I))\n",
    "        self.V = np.random.uniform(-0.01, 0.01, (K, I))\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        # helper function for numerically stable loss\n",
    "        def f(z):\n",
    "            return np.log1p(np.exp(-np.absolute(z))) + np.maximum(0, z)\n",
    "        \n",
    "        # infer dims\n",
    "        T, D = x.shape\n",
    "        K, I = self.V.shape\n",
    "\n",
    "        # init result arrays\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.a = np.zeros((T, I))\n",
    "        \n",
    "        # iterate forward in time \n",
    "        # trick: access model.a[-1] in first iteration\n",
    "        for t in range(T):\n",
    "            self.a[t] = np.tanh(self.W @ x[t] + self.R @ self.a[t-1])\n",
    "            \n",
    "        self.z = model.V @ self.a[t]\n",
    "        return y * f(-self.z) + (1-y) * f(self.z)\n",
    "\n",
    "T, D, I, K = 10, 3, 5, 1\n",
    "model = FullyRecurrentNetwork(D, I, K)\n",
    "model.forward(np.random.uniform(-1, 1, (T, D)), 1)\n",
    "\n",
    "import random \n",
    "\n",
    "def gen(T):\n",
    "    mu, sigma = 0, 0.2\n",
    "    for i in range(0,T):\n",
    "        yield np.random.normal(mu, sigma)\n",
    "\n",
    "\n",
    "def generate_data(T):\n",
    "    ########## YOUR SOLUTION HERE ##########\n",
    "    \n",
    "    x1 = np.fromiter(gen(T),dtype=float)\n",
    "    x2 = np.fromiter(gen(T),dtype=float)\n",
    "    \n",
    "    x1 = np.reshape(x1,newshape=(T,1))\n",
    "    x2 = np.reshape(x2,newshape=(T,1))\n",
    "\n",
    "    x1[0][0] = 1\n",
    "    y1 = np.array([1.0],dtype=float)\n",
    "\n",
    "    x2[0][0] = -1.0\n",
    "    y2 = np.array([0.0],dtype=float)\n",
    "\n",
    "    choice = np.random.choice([1,-1],1,p=[0.5,0.5])\n",
    "    if choice[0] == 1:\n",
    "        return x1,y1 #\n",
    "         \n",
    "    else:\n",
    "        return x2,y2 #\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd7c2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0xDEADBEEF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554c71f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = generate_data(5)\n",
    "print(\"x shape: \",x.shape)\n",
    "print(\"x[0]:\",x[0],\" y[0]:\",y[0])\n",
    "print(\"y shape: \",y.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a9826f26",
   "metadata": {},
   "source": [
    "## Exercise 2: Gradients for the network parameters\n",
    "Compute gradients of the total loss \n",
    "$$\n",
    "L = \\sum_{t=1}^T L(t), \\quad \\text{where} \\quad L(t) = L(z(t), y(t))\n",
    "$$\n",
    "w.r.t. the weights of the fully recurrent network. To this end, find the derivative of the loss w.r.t. the logits and hidden pre-activations first, i.e., \n",
    "$$\n",
    "\\psi^\\top(t) = \\frac{\\partial L}{\\partial z(t)} \\quad \\text{and} \\quad \\delta^\\top(t) = \\frac{\\partial L}{\\partial s(t)}.\n",
    "$$\n",
    "With the help of these intermediate results you should be able to compute the gradients w.r.t. the weights, i.e., $\\nabla_W L, \\nabla_R L, \\nabla_V L$. \n",
    "\n",
    "*Hint: Take a look at the computational graph from the previous assignment to see the functional dependencies.*\n",
    "\n",
    "*Remark: Although we only have one label at the end of the sequence, we consider the more general case of evaluating a loss at every time step in this exercise (many-to-many mapping).*\n",
    "\n",
    "########## YOUR SOLUTION HERE #########\n",
    "\n",
    "Let's remember how forward pass looks like:\n",
    "$$\n",
    "\\vec{s}(t) = \\vec{W}^\\top \\vec{x}(t) + \\vec{R}^\\top \\vec{a}(t-1) \\\\\n",
    "\\vec{a}(t) = f(\\vec{s}(t)) \\\\\n",
    "\\vec{z}(t) = \\vec{V}^\\top \\vec{a}(t) \\\\\n",
    "\\hat{\\vec{y}}(t) = \\phi(\\vec{z})\n",
    "$$\n",
    ", where $f$ is an $tanh$ - activation function and $\\phi$ is $sigmoid$ - output activation function, which depends on the task (not on the architecture)\n",
    "\n",
    "For the convinience of the future calculations we have to define derivative of the loss w.r.t. pre-activation function $\\vec{s}(t)$, which is called delta-error $\\vec{\\delta}$.\n",
    " \n",
    "Delta can be defined as following:\n",
    "$$\n",
    "\\vec{\\delta}(t)^\\top = \\frac{\\partial \\vec{L}}{\\partial \\vec{s}(t)} = \n",
    "\\frac{\\partial\\vec{L}}{\\partial\\vec{a}(t)}\\frac{\\partial\\vec{a}(t)}{\\partial\\vec{s}(t)} = \\\\ [20pt]\n",
    "\n",
    "\\left(\\frac{\\partial\\vec{L}\\left(\\vec{y}(t),\\hat{\\vec{y}}(t)\\right)}{\\partial\\vec{a}(t)} + \\frac{\\partial\\vec{L}}{\\partial\\vec{s}(t+1)} \\frac{\\partial\\vec{s}(t+1)}{\\partial\\vec{a}(t)}\\right)\\frac{\\partial{\\vec{a}(t)}}{\\partial{\\vec{s}(t)}} = \\\\[20pt]\n",
    "\n",
    "\\left(\\frac{\\partial\\vec{L}}{\\partial\\hat{\\vec{y}}(t)}\\frac{\\partial\\hat{\\vec{y}}(t)}{\\partial\\vec{z}(t)}\\frac{\\partial\\vec{z}(t)}{\\partial\\vec{a}(t)} + \\frac{\\partial\\vec{L}}{\\partial\\vec{s}(t+1)} \\frac{\\partial\\vec{s}(t+1)}{\\partial\\vec{a}(t)}\\right)\\frac{\\partial{\\vec{a}(t)}}{\\partial{\\vec{s}(t)}}\\\\ [20pt]\n",
    "$$\n",
    "$\\\\$ Here where it becomes tricky. From the last assignment, we remember that in practice, instead of separate prediction value $\\hat{\\vec{y}}$,  *logit*  $\\vec{z}(t)$ is directly inserted into loss function: $\\\\ \\vec{L}_{\\text{BCE stable}}(\\vec{z}(t), \\vec{y}(t)) =  \\log(1+e^{-|z|})+\\max(0, z)(1-y)$ . \n",
    "\n",
    "So, $\\frac{\\partial\\vec{L}}{\\partial\\hat{\\vec{y}}(t)}\\frac{\\partial\\hat{\\vec{y}}(t)}{\\partial\\vec{z}(t)}$ can be simplified to: $\\frac{\\partial L_{\\text{BCE stable}}(\\vec{z}(t), \\vec{y}(t))}{\\partial \\vec{z}(t)}$, which equals to:\n",
    "\n",
    "$$ \n",
    "\\frac{\\partial L_{\\text{BCE stable}}(\\vec{z}(t), \\vec{y}(t))}{\\partial \\vec{z}(t)} = \\frac{\\partial \\log(1+e^{-|\\vec{z}(t)|})+\\max(0, \\vec{z}(t))(1-y)}{\\partial \\vec{\\vec{z}(t)}} = - \\frac{sgn(\\vec{z}(t))}{1+e^{\\vec{z}(t)}} + \\frac{\\max(0, \\vec{z}(t))}{\\vec{z}(t)}(1-y)\n",
    "$$\n",
    "We will not, however, put that whole expression into our calculation of delta, but for the sake of simplicity let it be written  as $\\psi(t)^\\top$ . Therefore:\n",
    "\n",
    "$$\n",
    "... \n",
    "\\left(\\frac{\\partial L_{\\text{BCE stable}}(\\vec{z}(t), \\vec{y}(t))}{\\partial \\vec{z}(t)}\\frac{\\partial\\vec{z}(t)}{\\partial\\vec{a}(t)} + \\frac{\\partial\\vec{L}}{\\partial\\vec{s}(t+1)} \\frac{\\partial\\vec{s}(t+1)}{\\partial\\vec{a}(t)}\\right)\\frac{\\partial{\\vec{a}(t)}}{\\partial{\\vec{s}(t)}} = \\\\ [20pt]\n",
    "\n",
    "\\left( \\psi(t)^\\top \\vec{V}^\\top + \\vec{\\delta}(t+1)^\\top \\vec{R}^\\top \\right)diag\\left(f^\\prime(\\vec{s}(t)) \\right) \\\\[20pt]\n",
    "$$\n",
    "Whereas $f^\\prime(\\vec{s}(t)) = \\frac{\\partial\\, tanh(\\vec{s}(t)) }{\\partial\\, \\vec{s}(t)} = \\frac{1}{cosh^2(\\vec{s}(t))}$\n",
    "\n",
    "Using delta we can easily calculate gradients w.r.t. weights:\n",
    "$$\n",
    "\\nabla_{\\vec{R}} \\vec{L} = \\sum_{t=1}^T \\frac{\\partial \\vec{L}}{\\partial\\vec{s}(t)}\\frac{\\partial\\vec{s}(t)}{\\partial\\vec{R}} = \\sum_{t=1}^T \\vec{\\delta}(t)^\\top \\frac{\\partial \\left(\\vec{W}^\\top \\vec{x}(t) + \\vec{R}^\\top \\vec{a}(t-1) \\right)}{\\partial \\vec{R}} =\n",
    "\\sum_{t=1}^T \\vec{\\delta}(t)^\\top \\vec{a}(t-1) \\\\ [20pt]\n",
    "\n",
    "\\nabla_{\\vec{W}} \\vec{L} =  \\sum_{t=1}^T \\frac{\\partial \\vec{L}}{\\partial\\vec{s}(t)}\\frac{\\partial\\vec{s}(t)}{\\partial\\vec{W}} = \\sum_{t=1}^T \\vec{\\delta}(t)^\\top \\frac{\\partial \\left(\\vec{W}^\\top \\vec{x}(t) + \\vec{R}^\\top \\vec{a}(t-1) \\right)}{\\partial \\vec{W}} =\n",
    "\\sum_{t=1}^T \\vec{\\delta}(t)^\\top \\vec{x}(t) \\\\ [20pt]\n",
    "$$\n",
    "\n",
    "To find gradient for $\\vec{V}$ we shouldn't fo deep down untill $\\vec{s}(t)$, because $\\vec{V}$ can calculated using $\\psi(t)^\\top$ .  \n",
    "$$\n",
    "\\vec{\\psi}^\\top(t) = \\frac{\\partial L_{\\text{BCE stable}}(\\vec{z}(t), \\vec{y}(t))}{\\partial \\vec{z}(t)} = \\frac{\\partial L_{\\text{BCE stable}}(\\vec{z}(t), \\vec{y}(t))}{\\partial\\vec{V}^\\top\\; \\vec{a}(t)}\n",
    "$$\n",
    "\n",
    "\n",
    "Multiplying this definition with $\\vec{a}(t)$ will cancel out $\\vec{a}(t)$ in denominator:\n",
    "$$\n",
    "\\nabla_{\\vec{V}} \\vec{L} = \\sum_{t=1}^T \\frac{\\partial L_{\\text{BCE stable}}(\\vec{z}(t), \\vec{y}(t))}{\\partial\\vec{V}^\\top\\; \\cancel{\\vec{a}(t)}} \\cancel{\\;\\vec{a}(t)} = \\sum_{t=1}^T \\vec{\\psi}(t)\\;\\vec{a}(t)^\\top\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21968f2c",
   "metadata": {},
   "source": [
    "## Exercise 3: The backward pass\n",
    "Write a function `backward` that takes a model `self` as argument. The function should compute the gradients of the loss with respect to all model parameters and store them to `self.dW`, `self.dR`, `self.dV`, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42e3d13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(self): \n",
    "    T = len(self.z)\n",
    "    psi = np.zeros(T,)\n",
    "    delta = np.zeros(T,)\n",
    "    gradR = 0\n",
    "    gradW = 0\n",
    "    gradV = 0\n",
    "\n",
    "    for t in reversed(range(0, T)):\n",
    "        # calculate psi[t]\n",
    "        frac = - self.z[t]/( np.abs(self.z[t]) * (1+np.exp(self.z[t])))\n",
    "        if self.z[t] > 0:\n",
    "            frac += (1 - self.y)\n",
    "        psi[t] = frac\n",
    "\n",
    "        # calculate s[t] \n",
    "        s_t = self.W @ self.x[t]\n",
    "        if t > 0: # avoid out of range for self.a\n",
    "            s_t += self.R @ self.a[t-1]\n",
    "\n",
    "        # calculate delta[t] \n",
    "        dL_da = self.V.T @ [psi[t]]\n",
    "        da_ds = 1/(np.cosh(s_t))**2\n",
    "        if t != (len(self.z) - 1): # in case we aren't at the beginng and already have t+1 step ahead\n",
    "            dL_da += delta[t+1] # so we can add \"previous\" delta, since we're going from T-1 to 0 \n",
    "        delta[t] = dL_da @ da_ds\n",
    "    \n",
    "    # calculate gradients \n",
    "    for t in range(0, T):\n",
    "        #calculate gradR\n",
    "        if t!=0:       \n",
    "            gradR += delta[t] * self.a[t-1]\n",
    "        # calculate gradW\n",
    "        gradW += delta[t] * self.x[t]\n",
    "        # calculate gradV\n",
    "        gradV += psi[t] * self.a[t] \n",
    "    \n",
    "FullyRecurrentNetwork.backward = backward\n",
    "model.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58181c6",
   "metadata": {},
   "source": [
    "## Exercise 4: Gradient checking\n",
    "Write a function `grad_check` that takes a model `self`, a float `eps` and another float `thresh` as arguments and computes the numerical gradients of the model parameters according to the approximation\n",
    "$$\n",
    "f'(x) \\approx \\frac{f(x + \\varepsilon) - f(x - \\varepsilon)}{2 \\varepsilon}.\n",
    "$$\n",
    "If any of the analytical gradients are farther than `thresh` away from the numerical gradients the function should throw an error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227e8631",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_check(self, eps, thresh):\n",
    "    ########## YOUR SOLUTION HERE ##########\n",
    "\n",
    "FullyRecurrentNetwork.grad_check = grad_check\n",
    "model.grad_check(1e-7, 1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5cdc05",
   "metadata": {},
   "source": [
    "## Exercise 5: Parameter update\n",
    "\n",
    "Write a function `update` that takes a model `self` and a float argument `eta`, which represents the learning rate. The method should implement the gradient descent update rule $\\theta \\gets \\theta - \\eta \\nabla_{\\theta}L$ for all model parameters $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93c02c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(self, eta):\n",
    "    ########## YOUR SOLUTION HERE ##########\n",
    "\n",
    "FullyRecurrentNetwork.update = update\n",
    "model.update(0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19801bb8",
   "metadata": {},
   "source": [
    "## Exercise 6: Network training\n",
    "\n",
    "Train the fully recurrent network with 32 hidden units. Start with input sequences of length one and tune the learning rate and the number of update steps. Then increase the sequence length by one and tune the hyperparameters again. What is the maximal sequence length for which the fully recurrent network can achieve a performance that is better than random? Visualize your results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9781ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## YOUR SOLUTION HERE ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b52057",
   "metadata": {},
   "source": [
    "## Exercise 7: The Vanishing Gradient Problem\n",
    "\n",
    "Analyze why the network is incapable of learning long-term dependencies. Show that $\\|\\frac{\\partial a(T)}{\\partial a(1)}\\|_2 \\leq \\|R\\|_2^{T-1}$ , where $\\|\\cdot\\|_2$ is the spectral norm, and discuss how that affects the propagation of error signals through the time dimension of the network. \n",
    "\n",
    "*Hint: Use the fact that the spectral norm is submultiplicative for square matrices, i.e. $\\|AB\\|_2 \\leq \\|A\\|_2\\|B\\|_2$ if $A$ and $B$ are both square.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158130a5",
   "metadata": {},
   "source": [
    "########## YOUR SOLUTION HERE ##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e50719",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7 (default, Sep 16 2021, 08:50:36) \n[Clang 10.0.0 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
