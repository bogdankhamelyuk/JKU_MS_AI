{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: Introduction to the Fully Recurrent Network\n",
    "\n",
    "*Author:* Thomas Adler\n",
    "\n",
    "*Copyright statement:* This  material,  no  matter  whether  in  printed  or  electronic  form,  may  be  used  for  personal  and non-commercial educational use only.  Any reproduction of this manuscript, no matter whether as a whole or in parts, no matter whether in printed or in electronic form, requires explicit prior acceptance of the authors.\n",
    "\n",
    "\n",
    "## Exercise 1: Numerical stability of the binary cross-entropy loss function\n",
    "\n",
    "We will use the binary cross-entropy loss function to train our RNN, which is defined as \n",
    "$$\n",
    "L_{\\text{BCE}}(\\hat y, y) = -y \\log \\hat y - (1-y) \\log (1-\\hat y),\n",
    "$$\n",
    "where $y$ is the label and $\\hat y$ is a prediction, which comes from a model (e.g. an RNN) and is usually sigmoid-activated, i.e., we have\n",
    "$$\n",
    "\\hat y = \\sigma(z) = \\frac{1}{1+e^{||-z||}}.\n",
    "$$\n",
    "The argument $z$ is called *logit*. For reasons of numerical stability it is better to let the model emit the logit $z$ (instead of the prediction $\\hat y$) and incorporate the sigmoid activation into the loss function. Explain why this is the case and how we can gain numerical stability by combining the two functions $L_{\\text{BCE}}(\\hat y, y)$ and $\\sigma(z)$ into one function $L(z, y) = L_{\\text{BCE}}(\\sigma(z), y)$. \n",
    "\n",
    "*Hint: Prove that $\\log(1+e^{z}) = \\log (1+e^{-|z|}) + \\max(0, z)$ and argue why the right-hand side is numerically more stable. Finally, express $L(z,y)$ in terms of that form.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########## YOUR SOLUTION HERE ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Prove that $\\log(1+e^{z}) = \\log (1+e^{-|z|}) + \\max(0, z)$\n",
    "\n",
    "Let's do it:\n",
    "$$\n",
    "\\log(1+e^{z}) = \\log (1+e^{-|z|}) + \\max(0, z)\n",
    "$$\n",
    "remember, that $e^{|-z|} = \\frac{1}{e^{z}}$ and $\\log(\\frac{a}{b}) = \\log(a)-\\log(b)$ , so  \n",
    "$$\n",
    "\\log (1+e^{-|z|}) = \\log (1+ \\frac{1}{e^{z}}) = \\log (\\frac{e^{z}+1}{e^{z}})= \\log(e^{z}+1) - \\log(e^{z}) =\\log(e^{z}+1) - {z} \n",
    "$$\n",
    "\n",
    "Substituting $\\log (1+e^{-|z|})$ in the original expression through the calculation above results in the following:\n",
    "$$\n",
    "\\log(1+e^{z}) = \\log(e^{z}+1) - {z} + \\max(0,z)\n",
    "$$\n",
    "assuming that $\\max(0,z)$ results in most cases in $z$, so $- {z} + \\max(0,z)$ are excluding each other, what results in:\n",
    "$$\n",
    "\\log(1+e^{z}) = \\log(e^{z}+1)\n",
    "$$\n",
    "\n",
    ">argue why the right-hand side is numerically more stable\n",
    "\n",
    "Right-hand side is numerically more stable due to its linearity element expressed through $\\max(0, z)$.\n",
    "\n",
    ">Finally, express $L(z,y)$ in terms of that form.\n",
    "\n",
    "Now let's completely express BCE loss function in terms of $\\hat y = \\sigma(z) = \\frac{1}{1+e^{|-z|}}$:\n",
    "$$\n",
    "L_{\\text{BCE}}(z, y) = -y \\log( \\frac{1}{1+e^{|-z|}}) - (1-y) \\log (1-\\frac{1}{1+e^{|-z|}}) = \n",
    "$$\n",
    "\n",
    "$$\n",
    "=-y(\\log(1)-\\log(1+e^{|-z|}))-(1-y)\\log(\\frac{1+e^{|-z|}-1}{1+e^{|-z|}})=\n",
    "$$\n",
    "\n",
    "$$\n",
    "= -y(0-\\log(1+e^{|-z|}))-(1-y)\\log(\\frac{e^{|-z|}}{1+e^{|-z|}}) =\n",
    "$$\n",
    "\n",
    "$$\n",
    "= -y(-\\log(1+e^{|-z|}))-(1-y)(\\log(e^{|-z|}) - \\log(1+e^{|-z|}))= \n",
    "$$\n",
    "\n",
    "$$\n",
    "= -y(-\\log(1+e^{|-z|}))-(1-y)(-z - \\log(1+e^{|-z|}))\n",
    "$$\n",
    "let's express $\\log(1+e^{|-z|})$ so that $-z$ will become positive:\n",
    "$$\n",
    "-y(-\\log(1+e^{z})+z)-(1-y)(-z - \\log(1+e^{z})+z)=  \n",
    "$$\n",
    "$$\n",
    "=-y(-\\log(1+e^{z})+z)-(1-y)(- \\log(1+e^{z}))\n",
    "$$\n",
    "Now we can susbtitute $\\log(1+e^{z})$ through the expression above given in the hint:\n",
    "$$\n",
    "=-y(-\\log (1+e^{-|z|}))-(1-y)(- \\log (1+e^{-|z|}) - \\max(0, z))=\n",
    "$$\n",
    "$$\n",
    "=-y(-\\log(1+e^{-|z|}))+(1-y)(\\log(1+e^{-|z|})+\\max(0, z))=\n",
    "$$\n",
    "$$\n",
    "=y\\log(1+e^{-|z|})+\\log(1+e^{-|z|})-y\\log(1+e^{-|z|})+\\max(0, z)-y\\max(0, z)=\n",
    "$$\n",
    "$$\n",
    "=\\log(1+e^{-|z|})+\\max(0, z)(1-y)\n",
    "$$\n",
    "\n",
    "\n",
    "Now we can implement this function to see how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logit: 183 target:1 | L: 0.0\n",
      "logit: 484 target:0 | L: 484.0\n",
      "logit: 759 target:1 | L: 0.0\n",
      "logit: 424 target:1 | L: 0.0\n",
      "logit: 571 target:0 | L: 571.0\n",
      "logit: 576 target:0 | L: 576.0\n",
      "logit: 452 target:1 | L: 0.0\n",
      "logit: 338 target:0 | L: 338.0\n",
      "logit: 906 target:0 | L: 906.0\n",
      "logit: 650 target:0 | L: 650.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def num_stability_comparison(logits, targets):\n",
    "    for z, y in zip(logits,targets):\n",
    "        print(f\"logit: {z} target:{y} | L: {np.log(1+np.exp(-z))+max(0,z)*(1-y)}\")\n",
    "\n",
    "logits = np.random.randint(1000, size=10)\n",
    "targets = np.random.randint(2,size=10)\n",
    "\n",
    "num_stability_comparison(logits,targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Derivative of the loss\n",
    "\n",
    "Calculate the derivative of the binary cross-entropy loss function $L(z, y)$ with respect to the logit $z$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########## YOUR SOLUTION HERE ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Initializing the network\n",
    "Consider the fully recurrent network\n",
    "$$\n",
    "s(t) = W x(t) + R a(t-1) \\\\\n",
    "a(t) = \\tanh(s(t)) \\\\\n",
    "z(t) = V a(t) \\\\\n",
    "\\hat y(t) = \\sigma(z(t))\n",
    "$$\n",
    "for $t \\in \\mathbb{N}, x(t) \\in \\mathbb{R}^{D}, s(t) \\in \\mathbb{R}^{I}, a(t) \\in \\mathbb{R}^{I}, z(t) \\in \\mathbb{R}^K, \\hat y(t) \\in \\mathbb{R}^K$ and $W, R, V$ are real matrices of appropriate sizes and $\\hat a(0) = 0$. \n",
    "\n",
    "*Compared to the lecture notes we choose $f(x) = \\tanh(x) = (e^x - e^{-x})(e^x + e^{-x})^{-1}$ and $\\varphi(x) = \\sigma(x) = (1+e^{-x})^{-1}$. Further, we introduced an auxiliary variable $z(t)$ and transposed the weight matrices.*\n",
    "\n",
    "Write a function `init` that takes a `model` and integers $D, I, K$ as arguments and stores the matrices $W, R, V$ as members `model.W`, `model.R`, `model.V`, respectively. The matrices should be `numpy` arrays of appropriate sizes and filled with random values that are uniformly distributed between -0.01 and 0.01. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (1512276247.py, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_53646/1512276247.py\"\u001b[0;36m, line \u001b[0;32m14\u001b[0m\n\u001b[0;31m    Obj.init = init\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from scipy.special import expit as sigmoid\n",
    "\n",
    "class Obj(object):\n",
    "    pass\n",
    "\n",
    "model = Obj()\n",
    "T, D, I, K = 10, 3, 5, 1\n",
    "\n",
    "def init(model, D, I, K):\n",
    "    ########## YOUR SOLUTION HERE ##########\n",
    "\n",
    "Obj.init = init\n",
    "model.init(D, I, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: The forward pass\n",
    "Implement the forward pass for the fully recurrent network for sequence classification (many-to-one mapping). To this end, write a function `forward` that takes a `model`, a sequence of input vectors `x`, and a label `y` as arguments. The inputs will be represented as a `numpy` array of shape `(T, D)`. It should execute the behavior of the fully recurrent network and evaluate the (numerically stabilized) binary cross-entropy loss at the end of the sequence and return the resulting loss value. Store the sequence of hidden activations $(a(t))_{t=1}^T$ and the logit $z(T)$ into `model.a` and `model.z`, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (881695277.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_87968/881695277.py\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    Obj.forward = forward\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "def forward(model, x, y):\n",
    "    ########## YOUR SOLUTION HERE ##########\n",
    "\n",
    "Obj.forward = forward\n",
    "model.forward(np.random.uniform(-1, 1, (T, D)), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: The computational graph\n",
    "\n",
    "Visualize the computational graph of the fully recurrent network unfolded in time. The graph should show the functional dependencies of the nodes $x(t), a(t), z(t), L(z(t), y(t))$ for $t \\in \\{1, 2, 3\\}$. Use the package `networkx` in combination with `matplotlib` to draw a directed graph with labelled nodes and edges. If you need help take a look at [this guide](https://networkx.guide/visualization/basics/). Make sure to arrange the nodes in a meaningful way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "########## YOUR SOLUTION HERE ##########"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "bfbf927d9c9517429b938fe4dd7cbc6b9afc20ceffbcdf88c29908ac5b149b96"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
